---
layout: post
title: GloVe:Global Vectors for Word Representation
categories: [nlp]
---
이번 포스트는 [GloVe: Global Vectors for Word Representation(RS JeffreyPennington. 2014)](https://nlp.stanford.edu/pubs/glove.pdf)
을 읽고 정리한 글입니다.

## Introduction
* 의미론적 벡터 공간 모델에서는 각 단어를 하나의 실수 벡터로 표현한다.
* 많은 방법론들이 단어의 본질이 벡터로 잘 표현됐는지 평가하기 위해 거리, 각도 같은 벡터 연산을 이용한다.
* 그러나 최근 Mikolov et al.(2013)은 유추에 기반한 평가 체계를 도입해 모델의 차원들에 의미를 
반영하였고, 분산 표상(distributed representation)의 멀티 클러스터링 개념(Bengio, 2009)을 담아내었다.

$$
king - queen = man - woman \\
\text{("king is to queen as man is to woman")}
$$

* 오늘날 단어 벡터에 학습에 사용되는 두 가지 주요 모델들은 각각 심각한 결점을 갖고 있다.
    * 전체 행렬을 분해(factorization)하는 방법들(e.g. LSA): 통계 정보는 효과적으로 활용는 반면 유추에서는 성능이 좋지 않다.
    * 로컬 컨텍스트 윈도우를 사용하는 방법들(e.g. skip-gram): 유추에서는 나을 지 모르나, 코퍼스의 통계 정보가 반영되지 않는다.
* 본 연구에서는 모델이 벡터 공간 상에서 의미에 대한 선형적인 방향을 만들어내기 위해 필요한 속성들을 분석하고, 
말뭉치 전체에 대한 log-bilinear 회귀 모델이 이러한 속성들에 적합한지 논의한다. 
* 말뭉치 수준의 단어 간 동시 등장 횟수를 기반으로 학습하는 specific weighted least squares 모델을 제안하고,
효과적으로 통계정보를 활용할 수 있음을 보인다.

## Related Work
* Matrix Factorization Methods
* Shallow Window-Based Methods

## The Glove Model
### 문제인식
* 말뭉치에 등장하는 단어들에 대한 통계는 비지도학습에서 우선적으로 이용할 수 있는 정보이다.
* 그러나 이 정보에서 단어의 의미를 얻고, 그 의미를 결과 벡터에 반영하는 방법에 대해 여전히 의문이 있다.

### Notation
* $$X$$: 단어의 동시 발생 횟수 행렬
* $$X_{ij}$$: 행렬 $$X$$의 i, j번째 원소 ($$\sum_k X_{ik}$$: 단어 i의 문맥에서 임의의 단어가 등장한 총 횟수)
* $$P_{ij}$$: 동시 발생 확률. 단어 i의 문맥에서 단어 j가 나타날 확률 ($$=P(j \lvert i)=X_{ij}/X_i$$)

<div class="message">
<strong>NOTE</strong>
 : 단어의 동시 등장(co-occurence) 횟수 행렬 
<p>
단어의 동시 등장 횟수 행렬은 단어 i의 문맥 안에서 단어 j가 등장한 횟수를 나타낸 행렬을 의미한다.
</p>
<p>예시:</p>
<p style="margin-left:10px">
I like deep learning<br>
I like NLP<br>
I enjoy flying
</p>
<table style="margin-left: 10px;">
  <thead>
    <tr>
      <th></th>
      <th style="text-align: center">I</th>
      <th style="text-align: center">like</th>
      <th style="text-align: center">enjoy</th>
      <th style="text-align: center">deep</th>
      <th style="text-align: center">leanring</th>
      <th style="text-align: center">NLP</th>
      <th style="text-align: center">flying</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">I</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">like</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">enjoy</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">deep</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">learning</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">NLP</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">NLP</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>
</div>

### 아이디어
* 말뭉치의 통계를 직접적으로 반영한 새로운 단어 표현(word representation) 모델 GloVe를 고안하였다.
* 두 단어 i, j에 대해 관찰 단어 k의 동시 발생 확률의 비율($$P_{ik}/P_{jk}$$)을 이용하여 의미 관계를 추론할 수 있다는 점을 이용하였다.
    * $$P_{ik}/P_{jk} \gg 1$$: k가 i와 관련성이 높으나 j와는 낮은 경우
    * $$P_{ik}/P_{jk} \ll 1$$: k가 i와 관련성이 낮으나 j와는 높은 경우
    * $$P_{ik}/P_{jk} \approx 1$$: k가 두 단어 모두에 대해 관련성이 높거나 낮은 경우

**NOTE** : 동시 발생 확률에 대한 열역학 문서 예시<br><br>
열역학 문서에 두 단어 i=ice와 j=steam가 있다고 하자. 또 다른 단어 k=solid를 관찰(probe) 단어로 두고,
i, j가 k와 함께 등장할 확률인 $$P_{ik}$$와 $$P_{jk}$$의 비율을 계산해보면 두 단어의 관계를 고찰해 볼 수 있다. <br>
&nbsp;&nbsp;얼음은 고체이므로 ice와 solid는 함께 나타날 확률($$P_{ik})$$이 높을 것이고, 상대적으로 stream과 등장할 확률($$P_{jk}$$)이 낮을 것이다. k=gas로 두고 $$P_{ik}$$와 $$P_{jk}$$를 계산해보면 반대로 $$P_{ik}$$는 작은 값을 갖고 $$P_{jk}$$는 큰 값을 갖을 것이다. \\
&nbsp;&nbsp;k=water와 같이 i, j 둘 다와 관련성이 높은 단어면 어떻게 될까? $$P_{ik}$$와 $$P_{jk}$$의 값이 동시 커질 것이므로 동시 발생 확률에 대한 비율은 1에 가까워진다. 마지막으로 k=fashion으로 i, j 모두와 관련성이 낮은 단어라면, $$P_{ik}$$와 $$P_{jk}$$ 값이 둘 다 낮아지므로 이경우 역시 동시 발생 확률에 대한 비율은 1에 가까워 진다. \\
&nbsp;&nbsp;이로부터 두 단어가 관찰 단어에 대해 상대적이라면 동시 발생 확률의 비율 $$P_{ik}/P_{jk}$$는 1에서 멀어지고, 비슷한 관계인 경우에는 1에 가까워 진다는 것을 알 수 있다. 
{:.message}

### 단어 벡터의 학습
* 단어 i, j와 k가 주어졌을 때, 비율 $$ P_{ik}/P_{jk} $$을 예측하는 함수 $$F$$를 학습
* 단어 벡터 $$w \in \mathbb{R}^d $$, 문맥 단어 벡터 $$ \tilde{w} \in \mathbb{R}^d $$에 대해 아래 식으로 표현할 수 있다.

$$
F(w_i, w_j, \tilde{w}_k) = {P_{ik} \over P_{jk}} \qquad \qquad \ \ \ \ \ (1) \\
$$

* 단어 i, j의 상대적인 의미에 따른 $$ P_{ik}/P_{jk} $$ 의 변화를 반영하기 위하여,
아래와 같이 변형한다.

$$
F(w_i-w_j, \tilde{w}_k) = {P_{ik} \over P_{jk}}
$$

* 신경망과 같은 복잡한 함수는 선형 구조를 왜곡할 수 있으므로 함수 F를 스칼라 연산을 하도록 변형

$$
F((w_i - w_j)^T\tilde{w}_k) = {P_{ik} \over P_{jk}}
$$

* $$ w \leftrightarrow \tilde{w}, X \leftrightarrow X^T $$ 를 반영하기 위해,
$$ (\mathbb{R}, +) $$ 과 $$ (\mathbb{R_{>0}}, \times) $$ 에 대하여 homomorphism 을 만족하는
함수 F를 찾는다

$$
F((w_i - w_j)^T\tilde{w}_k) = F(w_i^T\tilde{w}_k - w_j^T\tilde{w}_k) = {F({w_i}^T \tilde{w}_k) \over F({w_j}^T \tilde{w}_k)}
$$

* $$F = exp$$ 라고 하면,

$$
exp(w_i^T\tilde{w}_k - w_j^T\tilde{w}_k) = {exp({w_i}^T \tilde{w}_k) \over exp({w_j}^T \tilde{w}_k)} = {F({w_i}^T \tilde{w}_k) \over F({w_j}^T \tilde{w}_k)}
$$

* 분자 $$ F(w_i^T \tilde{w}_k) $$는 정의에 의해,

$$
F(w_i^T \tilde{w}_k) = P_{ik} = {X_{ik} \over X_i}
$$

* $$ exp(w_i^T \tilde{w}_k) = F(w_i^T \tilde{w}_k) $$ 이므로 양변에 밑이 $$e $$인로그를 취하면

$$
w_i^T\tilde{w}_k = log(P_{ik}) = log(X_{ik}) - log(X_i)
$$

* $$X_{ki} = X_{ik}$$ 이므로, $$ log(X_i) \neq log(X_k) $$ 부분만 제외하고, k와 x가 서로 바뀌어도 같은 식을 갖게 된다.

$$ 
\tilde{w}_k^T w_i = log(X_{ki}) - log(X_k) 
$$

* $$ log(X_i) $$는 k에 대해 독립적이므로,  $$ log(X_i) = b_i $$ 로 바꿔 bias에 포함시킨 후,
교환법칙을 유지하기 위하여 $$ \tilde{b}_k $$ 를 추가한다. 


$$
J = \sum_{i,j=1}^V f(X_{ij}) (w_i^T \tilde{w}_j + b_i + \tilde{b}_j - logX_{ij})^2
$$

지금까지의 식은 동시 발생 확률의 비율을 회귀하는 것을 목표로 전개되었기 때문에,
절대적인 발생 횟수를 반영하지 못하는 문제가 있다. 따라서 발생 횟수 $$ X_{ij} $$ 에 대한
함수 $$ f(x_{ij}) $$를 가중치로 곱해줌으로써, 빈도를 반영하고자 한다.

### 동시 발생에 대한 가중치 함수
가중치 함수 $$ f(X_{ij}) $$는 다음과 같은 조건을 만족해야 한다:
1. $$ f(0) = 0 $$. $$ f $$가 연속함수 일 때, $$ \lim_{x \to 0} f(x)log^2 x $$가 유한할 수 있을만큼
빠르게 감소해야 한다.
2. $$ f(x) $$는 드물게 나타나는 경우에 대해 가중되지 않게 하기 위하여, 감소하지 않는 함수여야 한다.
3. $$ f(x) $$는 빈번한 경우가 너무 가중되지 않도록, 큰 x 값에 대해서는 상대적으로 작은 값을 가져야 한다.

본 연구에서는 잘 작동하는 함수의 한 가지로 아래 함수를 적용하였다.

$$
f(n)=
\begin{cases}
(x/x_{max})^\alpha & \text{if }x < x_{max} \\
1 & \text{otherwise} . 
\end{cases}
$$

모델의 성능이 어느 정도 $$ x_{max} $$에 영향을 받을 수 있으므로, 논문에서 모든 실험은 $$ x_{max} = 100 $$ 으로 진행하였으며, $$ \alpha = 3/4 $$ 인 경우에 적당한 성능을 보였다.

<figure>
  <img style="margin: 0 auto; width: 500px;" src="/assets/img/2020-03-08-Glove-1.png" />
  <figcaption>α = 3/4 일 때의 가중치 함수 f</figcaption>
</figure>







